###### CONCLUSIONS

уменьшаем вес слов, которые уже встречались в тексте
логирование в predict
время выполнения предикт
исправить тесты
сложное сглаживание (?)
модель для незнакомых слов
Сокращаем объем текста, пытаясь захватить большее количество уникальных слов
проверяем деплой
посмотреть N-grammy глазами поправить обработку текста, если надо
документация для функций
делаем презу










# ищем возможности улучшить модель
минимальная дистанция изменений, чтобы проверять нечеткое соответствие слов
обработка стеммером всех слов кроме last
ищем новые данные
обучаем модель верхнего уровня
? вероятности берем по логарифмической шкале
строим модели (skip-5-grams,skip-6-grams)


# Вопросы которые можно рассмотреть:
Почему в твиттере другое распределение ворд каунтов и иное количество уникальных слов.
Поискать еще источники

Процесс предсказания:
слова проверяются на соответствие по нечетким правилам
слово проверяется по всем справочникам - выдаются предупреждения
слово не находится в справочнике - Add a pseudo-word <UNK>
слово находится всправочнике:
слово или словосочетание ищется в индексе N-grams (возможно разделение по регионам)
находится
таблица сортируется по вероятности, выдаются топ 5 самых крутых предсказаний
не находится
ищем через модели незнакомых слов и N-grammov
человек начинает вводить новое слово
предсказание модифицируется фильтруя слова, начинающиеся с этой буквы и сортируя их по вероятности
