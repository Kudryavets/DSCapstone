###### CONCLUSIONS

посмотреть N-grammy глазами поправить обработку текста
<foreign> <foreign> >text ''

функция для обработки запроса

пишем рест

оценка модели

обработка стеммером всех слов кроме last

сложное сглаживание

минимальная дистанция изменений, чтобы проверять нечеткое соответствие слов

модель для незнакомых слов

Сокращаем объем текста, пытаясь захватить большее количество уникальных слов

строим модели (2,3,4,skip-5-grams,skip-6-grams)

обучаем модель верхнего уровня

? вероятности берем по логарифмической шкале

ищем новые данные
ищем возможности улучшить модель
делаем красивое представление ui


# Вопросы которые можно рассмотреть:
Почему в твиттере другое распределение ворд каунтов и иное количество уникальных слов.
Поискать еще источники

Процесс предсказания:
слова проверяются на соответствие по нечетким правилам
слово проверяется по всем справочникам - выдаются предупреждения
слово не находится в справочнике - Add a pseudo-word <UNK>
слово находится всправочнике:
слово или словосочетание ищется в индексе N-grams (возможно разделение по регионам)
находится
таблица сортируется по вероятности, выдаются топ 5 самых крутых предсказаний
не находится
ищем через модели незнакомых слов и N-grammov
человек начинает вводить новое слово
предсказание модифицируется фильтруя слова, начинающиеся с этой буквы и сортируя их по вероятности
