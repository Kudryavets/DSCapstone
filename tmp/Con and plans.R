###### CONCLUSIONS

документация для функций
restoring function

делаем презу

Сокращаем объем текста, пытаясь захватить большее количество уникальных слов (Remove singletons of higher-order n-grams, Quan*ze probabili*es (4-8 bits instead of 8-byte float))
посмотреть N-grammy глазами поправить обработку текста, если надо
сторим словарь


# ищем возможности улучшить модель
минимальная дистанция изменений, чтобы проверять нечеткое соответствие слов
обработка стеммером всех слов кроме last
ищем новые данные
обучаем модель верхнего уровня
? вероятности берем по логарифмической шкале
строим модели (skip-5-grams,skip-6-grams)


# Вопросы которые можно рассмотреть:
Почему в твиттере другое распределение ворд каунтов и иное количество уникальных слов.
Поискать еще источники

Процесс предсказания:
слова проверяются на соответствие по нечетким правилам
слово проверяется по всем справочникам - выдаются предупреждения
слово не находится в справочнике - Add a pseudo-word <UNK>
слово находится всправочнике:
слово или словосочетание ищется в индексе N-grams (возможно разделение по регионам)
находится
таблица сортируется по вероятности, выдаются топ 5 самых крутых предсказаний
не находится
ищем через модели незнакомых слов и N-grammov
человек начинает вводить новое слово
предсказание модифицируется фильтруя слова, начинающиеся с этой буквы и сортируя их по вероятности
